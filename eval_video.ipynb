{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "from open_clip.tokenizer import HFTokenizer\n",
    "import av\n",
    "\n",
    "from videoechoclip.model import create_model_and_transforms\n",
    "from videoechoclip.utils import random_seed, pt_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=\"config\", version_base=None):\n",
    "    args = hydra.compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = \"cuda:0\"\n",
    "device = torch.device(args.device)\n",
    "\n",
    "random_seed(args.seed, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model & preprocess functions\n",
    "model, preprocess_train, preprocess_val = create_model_and_transforms(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a checkpoint\n",
    "checkpoint_path = \"./weights/checkpoint.pt\"\n",
    "checkpoint = pt_load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "sd = checkpoint[\"state_dict\"]\n",
    "if next(iter(sd.items()))[0].startswith(\"module\"):\n",
    "    sd = {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
    "\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = HFTokenizer(args.model.text.hf_tokenizer_name, context_length=args.model.text.context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_preprocess(video_path, transforms, num_frames):\n",
    "    num_framesx2 = num_frames * 2  # NOTE stride 2\n",
    "\n",
    "    container = av.open(os.path.abspath(os.path.expanduser(video_path)), mode=\"r\")\n",
    "\n",
    "    frames = []\n",
    "    for frame in container.decode(video=0):\n",
    "        rgb_frame = frame.to_rgb()\n",
    "        arr = rgb_frame.to_ndarray()  # (H, W, 3)\n",
    "        # arr = cv2.resize(arr, (224, 224))\n",
    "        frames.append(arr)\n",
    "\n",
    "        if len(frames) == num_framesx2:\n",
    "            break\n",
    "\n",
    "    container.close()\n",
    "\n",
    "    if num_frames is not None and len(frames) < num_framesx2:\n",
    "        # if video is too short, pad last frame\n",
    "        rgb_frames = np.stack(frames + [frames[-1] for _ in range(num_framesx2 - len(frames))], axis=0)[::2]  # (N, H, W, 3) # NOTE stride 2\n",
    "    else:\n",
    "        rgb_frames =  np.stack(frames, axis=0)[::2]  # (N, H, W, 3) # NOTE stride 2\n",
    "\n",
    "    video_tensor = transforms(list(rgb_frames), return_tensors=\"pt\")[\"pixel_values\"][0]  # (N, 3, H', W')\n",
    "\n",
    "    return video_tensor.unsqueeze(0).to(device)  # (1, 3, N, H', W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text_path, tokenizer):\n",
    "    with open(os.path.abspath(os.path.expanduser(text_path)), \"r\") as f:\n",
    "        report_text = f.read().strip()\n",
    "\n",
    "    text_tensor = tokenizer([report_text])  # (1, L)\n",
    "\n",
    "    return text_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    image1 = video_preprocess(\"./example1.mp4\", preprocess_val, num_frames=args.model.vision.num_frames)  # (1, 3, N, H', W')\n",
    "    text1 = text_preprocess(\"./example1.txt\", tokenizer)  # (1, L)\n",
    "\n",
    "    image2 = video_preprocess(\"./example2.mp4\", preprocess_val, num_frames=args.model.vision.num_frames)  # (1, 3, N, H', W')\n",
    "    text2 = text_preprocess(\"./example2.txt\", tokenizer)  # (1,\n",
    "\n",
    "    image_feature1 = model.encode_image(image1, normalize=True)  # (1, 512)\n",
    "    text_feature1 = model.encode_text(text1, normalize=True)  # (1, 512)\n",
    "\n",
    "    image_feature2 = model.encode_image(image2, normalize=True)  # (1, 512)\n",
    "    text_feature2 = model.encode_text(text2, normalize=True)  # (1, 512)\n",
    "\n",
    "image_features = torch.cat([image_feature1, image_feature2])  # (2, 512)\n",
    "text_features = torch.cat([text_feature1, text_feature2])  # (2, 512)\n",
    "\n",
    "similarity = image_features @ text_features.T  # (2, 2)\n",
    "print(\"similarity score:\\n\", np.round(similarity.detach().cpu().numpy().squeeze(), decimals=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vecho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
